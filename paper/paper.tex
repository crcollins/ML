% \documentclass[10pt,twocolumn]{article}
\documentclass[10pt]{article}
\usepackage{graphicx}       % include figures
\usepackage{float}          % align figures
\usepackage{amsmath}        % \pm
% \usepackage{indentfirst}
\usepackage{achemso}
\usepackage[bf]{caption}    % bold figure names
\usepackage{url}
\usepackage{color}
\usepackage{soul}
\usepackage{setspace}
\restylefloat{table}        % same page tables
\usepackage{chngpage}
\usepackage{multirow}
% \graphicspath{ {./figures/} }

\begin{document}


\title{Application of Machine Learning to Predict the Optoelectronic Properties of Benzobisazoles}
\author{Christopher Collins \qquad Aim\'{e}e Tomlinson\\
        \emph{Department of Chemistry and Biochemistry}\\
        \emph{University of North Georgia}\\
        \emph{Dahlonega, GA 30597}}

\maketitle

\begin{abstract}
\hl{Because there are always more molecules to test than time}, there is a need to quickly be able to rule out structures that are unsuitable for further research. The method presented in this paper takes advantage of common structural features of molecules to decrease the required computational expense that can be seen equivalent Density Functional Theory calculations. By using machine learning methods, the HOMO, LUMO, and band gap can be predicted to within 0.09 eV, 0.08 eV, and 0.10 eV respectively for the benzobisazole structures tested. These results also come with the added benefit of being 30,000,000 times faster than the equivalent DFT calculations.
\end{abstract}

% \doublespace
\section{Introduction}

There are many different molecule groups that are being looked at for use as organic solar cells \hl{[CITE]}. One such group is the benzobisazoles \hl{[CITE]}. To optimize the optoelectronic properties of molecules for use in solar cells, various electron donating and electron accepting groups are added to the structure to shift electron density. For testing all of the possible structures (even in small sets) this is not feasible in the lab. To reduce some of the time required, and to expand the total search space, computational methods such as density functional theory have been used to predict the properties of molecules fairly rapidly. Unfortunately, these calculations also have their own limits in the amount of structures that can be tested in a reasonable time frame.
 \hl{There are many possible molecules that could be used as organic solar cells, the problem is current organic solar cells have poor efficiency [CITE]. To attempt to search through all the possible compounds that could be used for organic solar cells computational methods are used to save time.}

Density functional theory calculations have been used extensively to be able to predict the properties at a nice medium between the accuracy of \emph{ab inito} and the speed of semi empirical methods. Unfortunately, while they are faster than \emph{ab inito} calculations, DFT calculations still take a lot longer than to be desired. This problem is compounded when one of the of the interests in computational chemistry is being able to test molecules more quickly than could be done in the lab.

To take on both of both of these problems, we will be looking machine learning methods to both be able to predict molecular properties, but with the goal that these calculations would be inexpensive but retain the same level of accuracy. In recent work, various machine learning methods have been shown to be fast ways to predict properties of many systems and specifically, has been shown to be a viable way to predict properties of molecules\cite{hansen_assessment_2013} \cite{montavon_machine_2013}.

For this study, we will be looking at using different machine learning methods in conjunction with Density Functional Theory calculations to be able to predict the highest occupied molecular orbital (HOMO), lowest unoccupied molecular orbital (LUMO), and the band gap. The end goal of such methods being that they are faster ways to predict properties, but yet still retain enough accuracy such that they can be used to make quantitative \hl{....} about which structures would be of the most interest.

There are many common functional groups and structural features that are seen repeated across many molecules, so the goal of this is to make use of the fact that those structures have already been computed many times before, and one might expect that on average there is a common effect of each given group that can be used such that it does not require recomputing all of those effects for ever single instance of the feature. (By using that similarity, we expect to make calculations faster.)


\section{Systems of Interest}

For this study, we will be looking at the optoelectronic properties of benzobisazoles, a class of molecules comprised of a three fused ring core. This core is crucial in the fact that it it is highly conjugated which allows for better electron transfer and consequently make them better as organic solar cells. In addition to the core of the benzobisazole there are four different directions from the core that can be extended with various rings and other conjugated systems to attempt to tune the properties of the molecule.

For the side chains, structures were picked based on their ability to tune electronic properties. In the case of the main backbone structures, they were all picked because they are all highly planar and highly conjugated. Both of these features to aid in electron transfer. For the various substiuents, they picked based off their known ability to tune electronic properties \hl{(repeating...)}. The properties of this study have been shown in previous research to be correlated with Hammett Parameters. \hl{cool story...} Some of the trends as seen in previous work are that adding electron donating groups to the vertical axis and electron withdrawing  groups to the horizontal axis lead to higher HOMO energies and lower LUMO which in turn leads to an overall lower band gap. Part of the goal of this study is to take these general trends and synthesize them into a model that imparts a useful numerical value.

\subsection{Naming Scheme}

Unfortunately, the machine learning algorithms used for this study do not natively understand molecular structures and instead only are able to function on vector forms. To rectify this, a common naming scheme was formed such that it could be readily converted into a vector form. This naming scheme exploits the planar nature of benzobisazoles as well as their cruxiformic structure. Because of this regularity, they can be readily be structured in such a way that they can be converted into a format that is easy to represent to the machine learning algorithms. The naming scheme is broken down into three separate components. These parts are the core, the sides, and the options.

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=4in]{figures/cores.pdf}
  \end{center}
  \caption{TXY and CXY Cores. The $R^{H1}$, $R^v$, and $R^{H2}$ positions correspond to the left chain, the two vertical chains, and the right chain respectively.}
  \label{fig:cores}
\end{figure}

The core part of the structure is composed of a single three fused ring system. All cores are identified by a combination of three letters. The first letter indicates whether or not the structure is Cis or Trans which correspond to the letters C and T respectively. The second letter in the core name is the symbol for the element that is in the core without the $\pi$ bond (for consistency in naming this will always be the top right atom). This can be one of Oxygen (O), Sulfur (S), Nitrogen (N), Phosporous (P), or Carbon (C). The final letter is the remaining element's symbol which can be N, P, or C (for consistency this will always be the bottom right atom).

The sides of the name are split into three groups based on the location in the structure. Going from the core orientation, these three groups are the left, center, and right sides. The left and right sides of the structure form the horizontal axis. The center group is the vertical axis. For simplicity, both of the groups along the vertical axis are the same chain as they emanate from the core.

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=4in]{figures/aryl.pdf}
  \end{center}
  \caption{List of all the Aryl Groups and their respective identifiers. The $R^1$ and $R^2$ positions list possible places for added substituents.}
  \label{fig:Aryl}
\end{figure}

These Aryl groups are specifically were chosen because they are all highly conjugated. These structures in particular have also been shown to be useful in the tuning of optoelectronic properties of benzobisazoles \hl{[CITE]. (NEEDS MORE)}

Each side is then broken down into components of Aryl, X groups, and R groups. Aryl groups are indicated by the numbers 2 through 9 as seen in Figure \ref{fig:Aryl}. Aryl groups form the main chain part of the structure, they are able to extend the length of the chain and some have places for R groups (4, 5, 6, 7) to allow for better tuning of the optoelectronic properties.

\begin{table}[H]
  \centering
  \caption{X/R-Groups}
  \begin{tabular}{lc}
    Label   & Name      \\
    \hline
    A/a     & Hydrogen  \\
    B/b     & Chlorine  \\
    C/c     & Bromine   \\
    D/d     & Cyano     \\
    E/e     & Alkyne    \\
    F/f     & Hydroxy   \\
    G/g     & Thiol     \\
    H/h     & Amine     \\
    I/i     & Methyl    \\
    J/j     & Phenyl    \\
    K/k     & TMS       \\
    L/l     & Methoxy   \\
    *       & Spacer    \\
    \hline
  \end{tabular}
  \label{tab:xrgroups}
\end{table}

The X and R groups were picked based on their electron withdrawing/donating characteristics. Just like with the Aryl groups, these functional groups have also been shown to aid in the tuning of the optoelectronic properties of molecules \hl{[CITE]}. Previous work with these structures has also shown that HOMO and LUMO values have a strong correlation with the Hammet parameters of each of these functional groups \hl{(include? REPEAT...)}.

X and R groups are both composed of the same set of substituents seen in Table \ref{tab:xrgroups}. X groups are the letters $A$ through $L$, and R groups are $a$ through $l$. The only difference between X and R groups is where they are placed in the chain. R-Groups are placed on in the $R^1$ and $R^2$ positions of Aryl groups, whereas X groups on the other hand attach to the ends of chains and terminate them. As can be seen in Figure \ref{fig:Aryl} R-groups come in pairs of two. These groups can either be the same or different\hl{(?)}. Because there are some Aryl groups that do not allow any R-groups there is an additional spacer R-group that is designated as *. It has no effect on the structure of the molecule, it is just used as a place holder when converting the names to a feature vector.

Starting from the core going outward, side chains take two main forms. They are either an X-group (which terminates the entire side) or they are a triplet of an Aryl group and two R-groups. If it is an Aryl group triplet, then the process can continue. (X-group $\vert$ [Aryl, R-group, R-group])

The final component of the naming scheme is a set of options. These are broken up into two main classes. The first pair of options are the polymer options and they indicate how many times the molecules is repeated along the horizontal (n) axis \hl{(chain?)} or the vertical (m) axis \hl{(chain?)}. The second set of options is a set of x, y, and z that are the number of times that the molecule is stacked in each direction\hl{(?)}.


\section{Feature Vectors}

Representations of molecular structures are \hl{asdsa}, unfortunately, this intuition of molecular structure is not understood by computers. Instead of holistic pictures of the entire structure, they are left with lists of atom coordinates and bond connections. \hl{This could possibly be done by images of molecular structures (MASSIVE amounts of data).}

To represent these structures to the computer, and consequently to the machine learning algorithms, they must be converted into some sort of a structured vector. Fortunately, starting from the naming scheme described simplifies a lot of this work in that it is very regular\hl{??}. It is also beneficial in that many of the components of the name are larger structural features instead of individual atoms which would require larger vectors \hl{(meaning more computation required)}.

Using the naming scheme described, there are multiple ways that these names could be converted into feature vectors for use in machine learning. These range from methods purely from the different structural combinations to methods that take into account the cruxiformic structure of the benzobisazoles. \hl{more}


\subsection{Naive}

The first, and most simplistic, is to take all the possible combinations and create a binary vector from it with each $1$ in a location representing a particular combination. From the regular structure of the benzobisazoles, this can be done at the Aryl group triplet level. For each Aryl group triplet as they extend out from the core, a boolean vector based on the components of the triplet is formed.

The problem with this approach is that is the size of the feature vector groups proportional to the length of the longest side chain. This can be especially problematic if some of the training molecules are much larger than the others. Another problem with this type of feature vector it does not exploit similarities in the structure. \hl{(one might expect a chlorine two steps away from the core to cause about the same effect as one one step away times some decay factor)}.


\subsection{Decay}
To counter act these problems a second feature vector was created that used a decay function \eqref{decay}. $A$, $H$, and $p$ are arbitrary constants used to adjust the rate of decay. This adjustment of feature structure allows the grouping similar components together which in turn gives an implicit normalization of specific component attributes (charge, Hammet parameters, electronegativity, and etc).

\begin{equation}\label{decay}
    V = \sum_i^n (A d_i^{-H})^{p}
\end{equation}

The bad thing about this kind of decay function is that it is not separable if $H \times p \le 1$. This would mean that there is no unique conversion between a given feature vector and its associated name/structure. \hl{In the case of using this method as a sieve, this is not an issue.}


\subsection{Decay with Length Corrections}

One problem with the first decay type feature vector is it makes the assumption that all the distances between are the same length. Since the effects of atoms are highly dependent on distance this sort of assumption is problematic for heterogeneous side groups. \eqref{decay2} \hl{Something something}. This normalizes all the lengths such that longer structures will increase the distance more than shorter structures.

\begin{equation}\label{decay2}
    V = \sum_i^n \left(A \left(\frac{d_i}{M_i}\right)^{-H} \right)^{p}
\end{equation}

$M_i$ is a normalizing factor for each segment in the chain. By doing this, all the lengths are added to the chain are stored relative to the length of the first element in the chain. The problem with this feature vector is that it is not separable.


\begin{figure}[H]
  \begin{center}
    \includegraphics[width=4.5in]{figures/features.pdf}
  \end{center}
  \caption{Visual Representations of the respective Feature Vector types. \textbf{1.} Naive. \textbf{2.} Decay. \textbf{3.} Decay with length corrections.}
  \label{fig:features}
\end{figure}

\subsection{Decay with Predictions}

\hl{INSERT LINEAR LUMO-HOMO PLOT?}

In attempt to exploit the relation between HOMO, LUMO, and band gap, another feature vector was constructed. The feature vector was composed of a combination of the decay vector plus the addition of the two properties that were not being trained for. This was done in two stages, the first stage involved running the machine learning calculations as normal and then storing those results.

$$ predictedHOMO = predict(Decay, HOMO) $$
$$ predictedLUMO = predict(Decay, LUMO) $$
$$ predictedGAP = predict(Decay, GAP) $$

The second stage involved concatenating the original feature vectors to the vectors of predicted HOMO, LUMO, and band gap values. For each new feature vector, the goal property's respective predicted value was left out to avoid skewing the results.

$$ newGAP = [Decay, predictedHOMO, predictedLUMO] $$
$$ newHOMO = [Decay, predictedLUMO, predictedGAP] $$
$$ newLUMO = [Decay, predictedGAP, predictedHOMO] $$
\hl{MAYBE ILLUSTRATIONS? instead of psuedocode...}

These feature vectors were then feed back into the machine learning algorithms to get the final results.

$$ finalHOMO = predict(Decay, newHOMO) $$
$$ finalLUMO = predict(Decay, newLUMO) $$
$$ finalGAP = predict(Decay, newGAP) $$

\section{Machine Learning Methods}

In the work by Montavon et al., it has been shown that different machine learning algorithms have had drastically different performances when used to predict molecular properties. Because of this, seven algorithms were picked for comparison. These were mean value, linear regression, linear ridged regression, tree regression, k-nearest neighbors, and support vector machines. The support vector machines method was then broken down into two methods, one using a Gaussian kernel, and the other using a Laplacian kernel. The mean value method is used as a baseline for all of the other methods with its interpretation being the error that would be gotten by just guessing.

\hl{???? information about the different methods used?}
% \subsection{Mean Value}
% \subsection{Linear Regression}
% \subsection{Linear Ridge Regression}
% \subsection{Tree Regression}
% \subsection{k-Nearest Neighbors}
% \subsection{Support Vector Machines}
% \subsubsection{Gaussian Kernel}
% $$k(x, x') = exp\left( - \frac{1}{2 \sigma^2} || x - x' ||^2 \right)$$
% \subsubsection{Laplacian Kernel}
% $$k(x, x') = exp\left( - \frac{1}{\sigma} | x - x' | \right)$$



 \begin{figure}[H]
   \begin{center}
     \includegraphics[width=5in]{figures/surf.png}
   \end{center}
   \caption{Example scan of test errors (eV) as a function of SVM hyperparameters C and $\gamma$. The minimum located in the center corresponding to the ideal pair of hyperparameter values to use for the machine learning model. Similar scan procedures were done for the hyperparameters of the other models to find the lowest testing error.}
   \label{fig:scan}
 \end{figure}

\section{Procedure}

Roughly 1100 benzobisazole structures were picked to to cover as many combinations of cores, sides, and options as possible. In terms of Aryl triplets, all of the structures comprised of at most 4 triplets per side. This allowed for a constant vector size for the naive feature vector. (Many of these structures were from prior studies of different features of benzobisazoles). Once they were built, the structures were optimized using Density Functional Theory (DFT) with B3LYP/6-31g*. After the optimization, the molecules were then ran with Time Dependent Density Functional Theory (TDDFT) again with B3LYP/6-31g*. \hl{Beyond the 1100 benzobisazole structures, about 1000 other structures were collected for comparison.}

Once all the DFT calculations were completed, the resulting log files were processed for the HOMO, LUMO, and Band Gap energy values. From there, the first three feature vectors were constructed (Naive, Decay, Decay with Distance Correction) and the resulting vectors were stored with the energy data.

Once all the data and feature vectors were collected, they were feed into the machine learning algorithms. For each feature vector, property, method set the methods relevant hyperparameters were optimized (C, $\gamma$, max depth, $\alpha$, n neighbors). Then using these optimized hyperparameters, the machine learning models were trained using k-folds for cross validation, specifically using 10 folds \hl{[CITE]}. All of the results were then tabulated and compared based on their mean absolute errors (MAE) calculated as the distance the ML predicted value was from DFT calculated value.


\section{Results}

 \begin{figure}[H]
   \begin{center}
     \includegraphics[width=5.5in]{figures/allbars2.pdf}
   \end{center}
   \caption{Results. The far left column of each of the graphs can be seen as a baseline value for the set. So from there, any values that are lower than that value are better (with the optimal value being 0.0 eV). For the numerical values, refer to the Appendix.}
   \label{fig:results}
 \end{figure}

\section{Conclusions}

Overall, the support vector machines with Gaussian kernel had the best performance across all of the machine learning methods and all of the feature vectors. This matches similarly to the results seen in \hl{.... [CITE]}; however, they found that the Laplacian kernel performed the best. The suspected reason for the difference in the two results is thought to be because the method used in that study involved single atoms as features, whereas this method was dealing with entire functional groups. The smoothness of the gaussian distribution is probably a better estimator in that (especially with something like a phenyl ring). For the SVM with Gaussian kernel, the optimized hyperparameters to be $C \approx 10$ and $\gamma \approx 0.05$ for all of the properties.

For the feature vectors, the decay was the best performing feature vector with the parameters $A = 1, H = 1, p = 1$. \hl{(0.71)} As well as having a better overall performance, it is also much faster than the naive feature vector because it has fewer components. This increased speed could especially be seen when training the data. This can be attributed to the time complexity of $O(n^2) or O(n^3)$ where n is the number of dimensions.

With the addition of predicted HOMO/LUMO/Band Gap energies for the decay feature vector, the performance for estimating the goal features increased by about 25\%. The performance improvement is mostly attributed to the linear reaction between the three values.

Unexpectedly, the decay function with distance corrections did not perform any better than the regular decay feature vector. In theory, this should have worked better with heterogeneous side chains, but its use seemed to only increase the MAE. We think that the cause of this is due to the lack of sampling of largely heterogeneous side chains. So perhaps, if the same tests were done with only heterogeneous side chains adding the correction factor might decrease the total error.

In the results there is a large increase in the performance of the linear models (linear regression and linear ridge regression) when using the decay vector with predictions. This can be largely accounted for the linear relation between the HOMO, the LUMO, and the Band Gap energy levels $$BandGap \approx 0.93 (LUMO - HOMO) - 0.15$$.

There were a few anomalies in the results. The most noticeable is the long tail distribution of errors. As one might expect, structures where features overlap ($A\_TON\_4ed\_A$, 0.5 eV) there were higher MAE values. Unfortunately, a few molecules within the set had unusually high band gap errors (ex: $8A\_TON\_A\_A$, 1.1 eV) and we have no explanation for why that occurred.

To run the original DFT and TDDFT calculations for these molecules took about 35,0000 CPU hours. Calculating the same properties using these machine learning methods took about 4 seconds for each property for all 1100 structures. Because of the way that machine learning algorithms work, the majority of the time spent for each of these calculations is spent in the training phase; fortunately, this calculation is only required once so by pre-trainting the models the total time required can be greatly reduced. Once the models have been trained, these calculations are often reduced to just matrix multiplications leading to it taking just 0.1 seconds to calculate each property for all 1100 structures.


\section{Future Work}

For possible future work, the first thing to do would be to collect more data. For this study, the majority of the structures were monomers with 1-4 rings out from the core. This could be expanded to include larger polymers and monomers, especially with an emphasis on having more heterogeneous side chains. The functional groups could also be expanded to include things such as furan, pyrrole, or other conjugated systems.

None of the feature vectors take into account the Kuhn expression for polymer chains. Because many of these machine learning algorithms use linear models to fit, they do not accurately represent the nonlinear decay of influence seen in long polymers. This leads to high errors when $n>1$ or $m>1$. The problem is not as pronounced when using SVM with Gaussian kernels because of how they decay \hl{(?)}. \hl{Some preliminary work was done with predicting n=1,2,3,4 and then fitting the Kuhn expression for long polymers. This additional correction seems to be a good way to account for polymerization of benzobisazoles.}


% \begin{table}[H]
%   \centering
%   \caption{TON\_n20}
%   \begin{tabular}{lrrr}
%             Type      & HOMO (eV) & LUMO (eV) & Band Gap (eV) \\
%     \hline\hline
%             Predicted & -5.59 & -2.36 & 2.73 \\
%             Corrected & -5.99 & -3.36 & 2.13 \\
%             Actual    & -5.96 & -3.48 & 2.17 \\
%     \hline\hline
%   \end{tabular}
%   \label{tab:TONn20}
% \end{table}

% \begin{table}[H]
%   \centering
%   \caption{TON\_m20}
%   \begin{tabular}{lrrr}
%             Type      & HOMO (eV) & LUMO (eV) & Band Gap (eV) \\
%     \hline\hline
%             Predicted & -5.61 & -2.34 & 2.73 \\
%             Corrected & -5.84 & -2.20 & 3.10 \\
%             Actual    & -5.73 & -2.16 & 3.14 \\
%     \hline\hline
%   \end{tabular}
%   \label{tab:TONm20}
% \end{table}

Another possible expansion of work would be to test with more properties of the molecules such as total energy, dipole, and etc. \hl{preliminary results have shown that the naive feature vector works the best with predicting total energy (117 hartree MAE with svm, 2800 hartree MAE with mean) (about 2x as effective). This could be because the total energy is more dependent on the structure as a whole rather than the localization to the core.}

As an end goal, it would be nice to generalize these machine learning methods such that they could be used for any structure. \hl{The naming scheme described would not generalize to other structures, but perhaps a similar treatment of functional groups would lend itself to similar results.}

The methods presented here would be used as a first line filter to rule out large amounts molecular structures that do not meet the desired requirements such that more time could be allocated only on the structures that show the most promise instead of relying on intuition. As can be seen by these methods speeds and relative accuracy, these machine learning methods work really well for this task. In the same amount of time that one could calculate a property of a structure using DFT the same property can be predicted here for millions of structures. From those structures one could imagine taking the best 1\% of the structures and running progressively more accurate methods.

\hl{In addition to the machine learning methods used in this study, there was also an attempt to use Neural Networks for the predictions. Unfortunately, the results were not satisfactory relative to the amount of time required (0.20 eV MAE). Drastic changes to the network made minimal differences to the final error. This is attributed to inexperience with proper methodologies with Neural Networks.}

\hl{Also, the methods mentioned in }\cite{hansen_assessment_2013} \hl{were tested and the results were not as good as any of the other name based features (0.25-0.35 eV). In addition to the accuracy problems, it had severe performance problems because it relies on $O(n^2)$ elements in each feature vector where $n$ is the number of atoms in the structure. To save time, only structures with $<45$ atoms total were tested, leaving about 450 structures. Attempting the full data set is not feasible with all the structures/atoms. By using all the structures and using PCA to reduce the dimensionality down from 52,000 to 500 retains 99.97\% of the variability of the data (0.24 $\pm$ 0.04 eV MAE). To run the full data set it is expected to take on the order of 10s to 100s of days. (plus SVM does not work well when features >> samples)}

\section{Acknowledgments}

\hl{David Yaron, Gregorie Montavon, Bucknell (Marcy), University of North Georgia Department of Chemistry and Biochemistry, ...}

\nocite{ruddigkeit_enumeration_2012}
\nocite{darley_beyond_2008}
\nocite{frisch_gaussian_2009}
\nocite{montavon_machine_2013}
\nocite{handley_dynamically_2009}
\nocite{hansen_assessment_2013}
\nocite{schutt_how_2013}
\nocite{reymond_exploring_2012}
\nocite{whitfield_computational_2013}
\nocite{hansen_assessment_2013-1}
\nocite{ra_dft_2008}
\nocite{martell_assessment_1997}
\nocite{montavon_learning_2012}
\nocite{domingos_few_2012}
\nocite{martin_benchmark_1997}
\nocite{handley_potential_2010}
\nocite{rupp_fast_2012}

\bibliography{paper}
\newpage
\appendix

\section{Results}

\begin{table}[H]
  \centering
  \caption{Band Gap Results}
  \begin{tabular}{llr}
            Feature Vector     & Method       & MAE (eV)                \\
    \hline\hline
    \multirow{7}{*}{Naive}     & Mean Value   & 0.5051 $\pm$ 0.044 \\
                               & Linear       & 0.2628 $\pm$ 0.019 \\
                               & Linear Ridge & 0.2571 $\pm$ 0.020 \\
                               & Tree         & 0.1429 $\pm$ 0.016 \\
                               & k-NN         & 0.2096 $\pm$ 0.018 \\
                               & SVM Gauss    & \textbf{0.1355 $\pm$ 0.011} \\
                               & SVM Laplace  & 0.1681 $\pm$ 0.013 \\
    \hline
    \multirow{7}{*}{Decay}     & Mean Value   & 0.5051 $\pm$ 0.044 \\
                               & Linear       & 0.2531 $\pm$ 0.022 \\
                               & Linear Ridge & 0.2506 $\pm$ 0.023 \\
                               & Tree         & 0.1318 $\pm$ 0.018 \\
                               & k-NN         & 0.2112 $\pm$ 0.017 \\
                               & SVM Gauss    & \textbf{0.1297 $\pm$ 0.011} \\
                               & SVM Laplace  & 0.1591 $\pm$ 0.011 \\
    \hline
    \multirow{7}{*}{Decay with Length Corrections} & Mean Value   & 0.5051 $\pm$ 0.044 \\
                               & Linear       & 0.2485 $\pm$ 0.020 \\
                               & Linear Ridge & 0.2466 $\pm$ 0.020 \\
                               & Tree         & 0.1492 $\pm$ 0.029 \\
                               & k-NN         & 0.2169 $\pm$ 0.013 \\
                               & SVM Gauss    & \textbf{0.1314 $\pm$ 0.011} \\
                               & SVM Laplace  & 0.1631 $\pm$ 0.011 \\
    \hline
    \multirow{7}{*}{Decay with Predictions} & Mean Value   & 0.5051 $\pm$ 0.044 \\
                               & Linear       & 0.1211 $\pm$ 0.011 \\
                               & Linear Ridge & 0.1174 $\pm$ 0.010 \\
                               & Tree         & 0.1369 $\pm$ 0.012 \\
                               & k-NN         & 0.1724 $\pm$ 0.015 \\
                               & SVM Gauss    & \textbf{0.1004 $\pm$ 0.010} \\
                               & SVM Laplace  & 0.1154 $\pm$ 0.009 \\
    \hline\hline
  \end{tabular}
  \label{tab:gapresults}
\end{table}




% dummy           0.5051634571 $\pm$ 0.0445987177
% linear          0.2628013865 $\pm$ 0.0191410131
% linear ridge    0.2571941447 $\pm$ 0.0201764172
% tree            0.1429020978 $\pm$ 0.0167680834
% neighbors       0.2096637025 $\pm$ 0.0181147991
% svm gauss       0.1355760073 $\pm$ 0.0119249721
% svm laplace     0.1681324515 $\pm$ 0.0130277607

% dummy           0.5051634571 $\pm$ 0.0445987177
% linear          0.2531408695 $\pm$ 0.0224904198
% linear ridge    0.2506620918 $\pm$ 0.0231378900
% tree            0.1318830228 $\pm$ 0.0188910719
% neighbors       0.2112675515 $\pm$ 0.0170114767
% svm gauss       0.1297476028 $\pm$ 0.0110853891
% svm laplace     0.1591951178 $\pm$ 0.0113296220

% dummy           0.5051634571 $\pm$ 0.0445987177
% linear          0.2485031607 $\pm$ 0.0200054080
% linear ridge    0.2466766131 $\pm$ 0.0209252773
% tree            0.1492425133 $\pm$ 0.0293509142
% neighbors       0.2169500774 $\pm$ 0.0136509896
% svm gauss       0.1314727141 $\pm$ 0.0116959757
% svm laplace     0.1631757401 $\pm$ 0.0117913187

% dummy           0.5051634571 $\pm$ 0.0445987177
% linear          0.1211359604 $\pm$ 0.0116286996
% linear ridge    0.1174973321 $\pm$ 0.0101995097
% tree            0.1369228897 $\pm$ 0.0123968003
% neighbors       0.1724752783 $\pm$ 0.0152423160
% svm gauss       0.1004123381 $\pm$ 0.0106025428
% svm laplace     0.1154254876 $\pm$ 0.0096740880


\begin{table}[H]
  \centering
  \caption{HOMO Results}
  \begin{tabular}{llr}
            Feature Vector     & Method       & MAE (eV)                \\
    \hline\hline
    \multirow{7}{*}{Naive} & Mean Value   & 0.4330 $\pm$ 0.038 \\
                               & Linear       & 0.2002 $\pm$ 0.024 \\
                               & Linear Ridge & 0.1949 $\pm$ 0.024 \\
                               & Tree         & 0.1546 $\pm$ 0.012 \\
                               & k-NN         & 0.2449 $\pm$ 0.025 \\
                               & SVM Gauss    & \textbf{0.1126 $\pm$ 0.014} \\
                               & SVM Laplace  & 0.1557 $\pm$ 0.016 \\
    \hline
    \multirow{7}{*}{Decay}     & Mean Value   & 0.4330 $\pm$ 0.038 \\
                               & Linear       & 0.1868 $\pm$ 0.025 \\
                               & Linear Ridge & 0.1860 $\pm$ 0.025 \\
                               & Tree         & 0.1455 $\pm$ 0.017 \\
                               & k-NN         & 0.2549 $\pm$ 0.026 \\
                               & SVM Gauss    & \textbf{0.1069 $\pm$ 0.010} \\
                               & SVM Laplace  & 0.1431 $\pm$ 0.014 \\
    \hline
    \multirow{7}{*}{Decay with Length Corrections} & Mean Value   & 0.4330 $\pm$ 0.038 \\
                               & Linear       & 0.1883 $\pm$ 0.024 \\
                               & Linear Ridge & 0.1875 $\pm$ 0.025 \\
                               & Tree         & 0.1453 $\pm$ 0.021 \\
                               & k-NN         & 0.2622 $\pm$ 0.028 \\
                               & SVM Gauss    & \textbf{0.1082 $\pm$ 0.012} \\
                               & SVM Laplace  & 0.1457 $\pm$ 0.017 \\
    \hline
    \multirow{7}{*}{Decay with Predictions} & Mean Value   & 0.4330 $\pm$ 0.038 \\
                               & Linear       & 0.0904 $\pm$ 0.007 \\
                               & Linear Ridge & 0.0897 $\pm$ 0.008 \\
                               & Tree         & 0.1439 $\pm$ 0.016 \\
                               & k-NN         & 0.1734 $\pm$ 0.020 \\
                               & SVM Gauss    & \textbf{0.0852 $\pm$ 0.007} \\
                               & SVM Laplace  & 0.1001 $\pm$ 0.008 \\
    \hline\hline
  \end{tabular}
  \label{tab:homoresults}
\end{table}


% dummy            0.4330672042 $\pm$ 0.0380947474
% linear           0.2002858544 $\pm$ 0.0244903400
% linear ridge     0.1949752271 $\pm$ 0.0248602704
% tree             0.1546552205 $\pm$ 0.0122826022
% neighbors        0.2449855565 $\pm$ 0.0258103540
% svm gauss        0.1126612407 $\pm$ 0.0141583310
% svm laplace      0.1557953600 $\pm$ 0.0169683266

% dummy            0.4330672042 $\pm$ 0.0380947474
% linear           0.1868573168 $\pm$ 0.0252679509
% linear ridge     0.1860609806 $\pm$ 0.0257993800
% tree             0.1455811816 $\pm$ 0.0175405978
% neighbors        0.2549135699 $\pm$ 0.0261735083
% svm gauss        0.1069055686 $\pm$ 0.0100414699
% svm laplace      0.1431763121 $\pm$ 0.0145504211

% dummy            0.4330672042 $\pm$ 0.0380947474
% linear           0.1883473946 $\pm$ 0.0248143868
% linear ridge     0.1875977704 $\pm$ 0.0251635444
% tree             0.1453857318 $\pm$ 0.0214142873
% neighbors        0.2622268718 $\pm$ 0.0287591181
% svm gauss        0.1082111822 $\pm$ 0.0124536000
% svm laplace      0.1457133829 $\pm$ 0.0170146564

% dummy            0.4330672042 $\pm$ 0.0380947474
% linear           0.0904848003 $\pm$ 0.0077179117
% linear ridge     0.0897196018 $\pm$ 0.0085006159
% tree             0.1439614181 $\pm$ 0.0167746654
% neighbors        0.1734307394 $\pm$ 0.0202218099
% svm gauss        0.0852044604 $\pm$ 0.0072159851
% svm laplace      0.1001281598 $\pm$ 0.0086522943

\begin{table}[H]
  \centering
  \caption{LUMO Results}
  \begin{tabular}{llr}
            Feature Vector     & Method       & MAE (eV)                \\
    \hline\hline
    \multirow{7}{*}{Naive}     & Mean Value   & 0.4475 $\pm$ 0.037 \\
                               & Linear       & 0.1900 $\pm$ 0.028 \\
                               & Linear Ridge & 0.1862 $\pm$ 0.025 \\
                               & Tree         & 0.1529 $\pm$ 0.007 \\
                               & k-NN         & 0.2351 $\pm$ 0.019 \\
                               & SVM Gauss    & \textbf{0.1055 $\pm$ 0.010} \\
                               & SVM Laplace  & 0.1454 $\pm$ 0.014 \\
    \hline
    \multirow{7}{*}{Decay}     & Mean Value   & 0.4475 $\pm$ 0.037 \\
                               & Linear       & 0.1816 $\pm$ 0.026 \\
                               & Linear Ridge & 0.1801 $\pm$ 0.026 \\
                               & Tree         & 0.1510 $\pm$ 0.024 \\
                               & k-NN         & 0.2323 $\pm$ 0.017 \\
                               & SVM Gauss    & \textbf{0.1022 $\pm$ 0.015} \\
                               & SVM Laplace  & 0.1317 $\pm$ 0.012 \\
    \hline
    \multirow{7}{*}{Decay with Length Corrections} & Mean Value   & 0.4475 $\pm$ 0.037 \\
                               & Linear       & 0.1847 $\pm$ 0.027 \\
                               & Linear Ridge & 0.1828 $\pm$ 0.027 \\
                               & Tree         & 0.1540 $\pm$ 0.023 \\
                               & k-NN         & 0.2348 $\pm$ 0.014 \\
                               & SVM Gauss    & \textbf{0.1029 $\pm$ 0.012} \\
                               & SVM Laplace  & 0.1331 $\pm$ 0.012 \\
    \hline
    \multirow{7}{*}{Decay with Predictions} & Mean Value   & 0.4475 $\pm$ 0.037 \\
                               & Linear       & 0.1233 $\pm$ 0.012 \\
                               & Linear Ridge & 0.0877 $\pm$ 0.011 \\
                               & Tree         & 0.0863 $\pm$ 0.010 \\
                               & k-NN         & 0.1699 $\pm$ 0.017 \\
                               & SVM Gauss    & \textbf{0.0792 $\pm$ 0.009} \\
                               & SVM Laplace  & 0.0922 $\pm$ 0.009 \\
    \hline\hline
  \end{tabular}
  \label{tab:lumoresults}
\end{table}


% dummy           0.4475990362 $\pm$ 0.0379190939
% linear          0.1900266591 $\pm$ 0.0285711677
% linear ridge    0.1862466873 $\pm$ 0.0252330179
% tree            0.1529660715 $\pm$ 0.0077728697
% neighbors       0.2351363937 $\pm$ 0.0193158199
% svm gauss       0.1055791858 $\pm$ 0.0104366108
% svm laplace     0.1454523271 $\pm$ 0.0145551515

% dummy           0.4475990362 $\pm$ 0.0379190939
% linear          0.1816689922 $\pm$ 0.0263603308
% linear ridge    0.1801892946 $\pm$ 0.0269365682
% tree            0.1510884685 $\pm$ 0.0242985650
% neighbors       0.2323162704 $\pm$ 0.0179171316
% svm gauss       0.1022628994 $\pm$ 0.0150820066
% svm laplace     0.1317870230 $\pm$ 0.0121778503

% dummy           0.4475990362 $\pm$ 0.0379190939
% linear          0.1847361405 $\pm$ 0.0279060679
% linear ridge    0.1828131087 $\pm$ 0.0270449435
% tree            0.1540660334 $\pm$ 0.0230038065
% neighbors       0.2348623132 $\pm$ 0.0142070847
% svm gauss       0.1029487972 $\pm$ 0.0123167244
% svm laplace     0.1331720123 $\pm$ 0.0123964303

% dummy           0.4475990362 $\pm$ 0.0379190939
% tree            0.1233738710 $\pm$ 0.0122788181
% linear          0.0877481316 $\pm$ 0.0110845291
% linear ridge    0.0863736416 $\pm$ 0.0108794424
% neighbors       0.1699337745 $\pm$ 0.0170310968
% svm gauss       0.0792332669 $\pm$ 0.0091141118
% svm laplace     0.0922304705 $\pm$ 0.0092304161

\section{Other}

The DFT calculations were done using the computing cluster \hl{Marcy from Bucknell} and Gaussian 09. The machine learning algorithms used were the ones implemented in the \hl{Scikit-Learn Python Library}. All of the machine learning calculations were done on a single consumer quad core computer.

OTHER
numpy, scipy, matplotlib

\end{document}
