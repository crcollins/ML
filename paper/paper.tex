% \documentclass[10pt,twocolumn]{article}
\documentclass[10pt]{article}
\usepackage{graphicx}       % include figures
\usepackage{float}          % align figures
\usepackage{amsmath}        % \pm
% \usepackage{indentfirst}
\usepackage{achemso}
\usepackage[bf]{caption}    % bold figure names
\usepackage{url}
\usepackage{color}
\usepackage{soul}
\usepackage{setspace}
\restylefloat{table}        % same page tables
\usepackage{chngpage}
\usepackage{multirow}
% \graphicspath{ {./figures/} }

\begin{document}


\title{Application of Machine Learning to Predict the Optoelectronic Properties of Benzobisazoles}
\author{Christopher Collins \qquad Aim\'{e}e Tomlinson\\
        \emph{Department of Chemistry and Biochemistry}\\
        \emph{University of North Georgia}\\
        \emph{Dahlonega, GA 30597}}

\maketitle

\begin{abstract}
Because there are always more molecules to test than time, there is a need to quickly be able to rule out structures that are unsuitable for further research. The method presented here takes advantage of common structural features of molecules to decrease the required computational expense that can be seen equivalent Density Functional Theory calculations. By using machine learning methods, the HOMO, LUMO, and band gap can be predicted to within 0.09 eV, 0.08 eV, and 0.10 eV respectively for the benzobisazole structures tested. These results also come with the added benefit of being 30,000,000 times faster than the equivalent DFT calculation.
\end{abstract}

% \doublespace
\section{Introduction}

There are many different molecule groups that are being looked at for use as organic solar cells \hl{[CITE]}. One such group is the benzobisazoles \hl{[CITE]}. There are many possible molecules that could be used as organic solar cells, the problem is current organic solar cells have poor efficiency \hl{[CITE]}. To attempt to search through all the possible compounds that could be used for organic solar cells computational methods are used to save time.

Density functional theory calculations have been used extensively to be able to predict the properties at a nice medium between the accuracy of \emph{ab inito} and the speed of semi empirical methods. Unfortunately, while they are faster than \emph{ab inito} calculations, DFT calculations still take a lot longer than to be desired. This problem is compounded when much of the interest in computational chemistry is being able to test molecules more quickly than could be done in the lab.

Because of these problems, there is a need to rapidly predict the properties of molecules so that. Machine learning has been shown to be a fast way to predict properties of many systems and specifically, has been shown to be a viable way to predict properties of molecules\cite{hansen_assessment_2013} \cite{montavon_machine_2013}.

For this study, we will be looking at using different machine learning methods in conjunction with Density Functional Theory calculations to be able to predict the highest occupied molecular orbital (HOMO), lowest unoccupied molecular orbital (LUMO), and the band gap. The end goal of such methods being that they are faster ways to predict properties, but yet still retain enough accuracy such that they can be used to make quantitative \hl{....} about which structures would be of the most interest.

There are many common features that are seen in many molecules, things like methyls, phenyls, and thiophenes so the goal of this is to make use of the fact that those structures have already been computed many times before, and one might expect that on average there is a common effect of each given group that can be used such that it does not require recomputing all of those effects for ever single instance of the feature. (By using that similarity, we expect to make calculations faster.)


\section{Systems of Interest}

For this study, we will be looking at the optoelectronic properties of benzobisazoles, a class of molecules comprised of a three fused ring core. This core is crucial in the fact that it it is highly conjugated which allows for better electron transfer and consequently make them better as organic solar cells. In addition to the core of the benzobisazole there are four different directions from the core that can be extended with various rings and other conjugated systems to attempt to tune the properties of the molecule.

For the side chains, structures were picked based on their ability to tune electronic properties. In the case of the main backbone structures, they were all picked because they are all highly planar and highly conjugated. Both of these features to aid in electron transfer. For the various substiuents, they picked based off their known ability to tune electronic properties \hl{(repeating...)}. The properties of this study have been shown in previous research to be correlated with Hammett Parameters. \hl{cool story...}

\subsection{Naming Scheme}

Unfortunately, the machine learning algorithms used for this study do not natively understand molecular structures and instead only are able to function on vector forms. To rectify this, a common naming scheme was formed such that it could be readily converted into a vector form. This naming scheme exploits the planar nature of benzobisazoles as well as their cruxiformic structure. Because of this regularity, they can be readily be structured in such a way that they can be converted into a format that is easy to represent to the machine learning algorithms. The naming scheme is broken down into three separate components. These parts are the core, the sides, and the options.

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=4in]{figures/cores.pdf}
  \end{center}
  \caption{TXY and CXY Cores}
  \label{fig:cores}
\end{figure}

The core part of the structure is composed of a single three fused ring system. All cores are identified by a combination of three letters. The first letter indicates whether or not the structure is Cis or Trans which correspond to the letters C and T respectively. The second letter in the core name is the symbol for the element that is in the core without the $\pi$ bond (for consistency in naming this will always be the top right atom). This can be one of Oxygen (O), Sulfur (S), Nitrogen (N), Phosporous (P), or Carbon (C). The final letter is the remaining element's symbol which can be N, P, or C (for consistency this will always be the bottom right atom).

The sides of the name are split into three groups based on the location in the structure. Going from the core orientation, these three groups are the left, center, and right sides. The left and right sides of the structure form the horizontal axis. The center group is the vertical axis. For simplicity, both of the groups along the vertical axis are the same chain as they emanate from the core.

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=4in]{figures/aryl.pdf}
  \end{center}
  \caption{Aryl Groups}
  \label{fig:Aryl}
\end{figure}

These Aryl groups are specifically were chosen because they are all highly conjugated. These structures in particular have also been shown to be useful in the tuning of optoelectronic properties of benzobisazoles \hl{[CITE]. (NEEDS MORE)}

Each side is then broken down into components of Aryl, X groups, and R groups. Aryl groups are indicated by the numbers 2 through 9 as seen in Figure \ref{fig:Aryl}. Aryl groups form the main chain part of the structure, they are able to extend the length of the chain and some have places for R groups (4, 5, 6, 7).

\begin{table}[H]
  \centering
  \caption{X/R-Groups}
  \begin{tabular}{lc}
    Label   & Name      \\
    \hline
    A/a     & Hydrogen  \\
    B/b     & Chlorine  \\
    C/c     & Bromine   \\
    D/d     & CN        \\
    E/e     & CCH       \\
    F/f     & OH        \\
    G/g     & SH        \\
    H/h     & $NH_2$    \\
    I/i     & $CH_3$    \\
    J/j     & Phenyl    \\
    K/k     & TMS       \\
    L/l     & $OCH_3$   \\
    *       & Spacer    \\
    \hline
  \end{tabular}
  \label{tab:xrgroups}
\end{table}

The X and R groups were picked based on their electron withdrawing/donating characteristics. Just like with the Aryl groups, these functional groups have also been shown to aid in the tuning of the optoelectronic properties of molecules \hl{[CITE]}. Previous work with these structures has also shown that HOMO and LUMO values have a strong correlation with the Hammet parameters of each of these functional groups \hl{(include?)}.


X and R groups are both composed of the same set of substituents seen in Table \ref{tab:xrgroups}. X groups are the letters $A$ through $L$, and R groups are $a$ through $l$. The main difference between X and R groups is where they are placed in the chain, whereas R-Groups are placed on the Aryl group that directly proceeds them. As can be seen in Figure \ref{fig:Aryl} R-groups come in pairs of two. These groups can either be the same or different\hl{(?)}. Because there are some Aryl groups that do not allow any R-groups there is an additional spacer R-group that is designated as *. It has no effect on the structure of the molecule, it is just used as a place holder when converting the names to a feature vector.

Starting from the core going outward, side chains take two main forms. They are either an X-group (which terminates the entire side) or they are a triplet of an Aryl group and two R-groups. If it is an Aryl group triplet, then the process can continue. (X-group $\vert$ [Aryl, R-group, R-group])

The final component of the naming scheme is a set of options. These are broken up into two main classes. The first pair of options are the polymer options and they indicate how many times the molecules is repeated along the horizontal (n) axis \hl{(chain?)} or the vertical (m) axis \hl{(chain?)}. The second set of options is a set of x, y, and z that are the number of times that the molecule is stacked in each direction\hl{(?)}.


\section{Feature Vectors}

To be able to represent these structures to the computer, and consequently to the machine learning algorithms they must be vectorized.

Using this naming scheme, it is apparent that there are multiple ways that these could be converted into feature vectors for use in machine learning.


\subsection{Naive}

The first, and most simplistic, is to take all the possible combinations and create a binary vector from it with each $1$ in a location representing a particular combination. From the regular structure of the benzobisazoles, this can be done at the Aryl group triplet level. For each Aryl group triplet as they extend out from the core, a boolean vector based on the components of the triplet is formed. The problem with this approach is that is the size of the feature vector groups proportional to the length of the longest side chain. This can be especially problematic if some of the training molecules are much larger than the others. Another problem with this type of feature vector it does not exploit similarities in the structure. (one might expect a chlorine two steps away from the core to cause about the same effect as one one step away times some decay factor).


\subsection{Decay}
To counter act these problems a second feature vector was created that used a decay function \eqref{decay}. $A$, $H$, and $p$ are arbitrary constants used to adjust the rate of decay. By grouping similar components together, it normalizes implicit features (charge, Hammet parameters, and etc).

\begin{equation}\label{decay}
    Value = \sum_i^n (A d_i^{-H})^{p}
\end{equation}

The bad thing about this kind of decay function is that it is not separable if $H \times p \le 1$. This would mean that there is no unique conversion between a given feature vector and its associated name/structure.


\subsection{Decay with Length Corrections}

One problem with the first decay type feature vector is it makes the assumption that all the distances between are the same length. Since the effects of atoms are highly dependent on distance this sort of assumption is problematic for heterogeneous side groups. \eqref{decay2} Something something. This normalizes all the lengths such that longer structures will increase the distance more than shorter structures.

$M_i$ is a normalizing factor for each segment in the chain. By doing this, all the lengths are normalized relative to the first element.

\begin{equation}\label{decay2}
    Value = \sum_i^n \left(A \left(\frac{d_i}{M_i}\right)^{-H} \right)^{p}
\end{equation}

The problem with this feature vector is that it is not separable.

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=4.5in]{figures/features.pdf}
  \end{center}
  \caption{Visual Representations of the respective Feature Vector types. \textbf{1.} Naive. \textbf{2.} Decay. \textbf{3.} Decay with length corrections.}
  \label{fig:features}
\end{figure}

\subsection{Decay with Predictions}

In attempt to exploit the relation between HOMO, LUMO, and band gap, another feature vector was constructed. The feature vector was composed of a combination of the decay vector plus the addition of the two properties that were not being trained for. This was done in two stages, the first stage involved running the machine learning calculations as normal and then storing those results.

$$ predictedHOMO = predict(Decay, HOMO) $$
$$ predictedLUMO = predict(Decay, LUMO) $$
$$ predictedGAP = predict(Decay, GAP) $$

The second stage involved concatenating the original feature vectors to the vectors of predicted HOMO, LUMO, and band gap values. For each new feature vector, the goal property's respective predicted value was left out to avoid skewing the results.

$$ newGAP = [Decay, predictedHOMO, predictedLUMO] $$
$$ newHOMO = [Decay, predictedLUMO, predictedGAP] $$
$$ newLUMO = [Decay, predictedGAP, predictedHOMO] $$

These feature vectors were then feed back into the machine learning algorithms to get the final results.

$$ finalHOMO = predict(Decay, newHOMO) $$
$$ finalLUMO = predict(Decay, newLUMO) $$
$$ finalGAP = predict(Decay, newGAP) $$

\section{Machine Learning Methods}

In previous work \cite{montavon_machine_2013}, it has been shown that different machine learning algorithms have had drastically different performance when used to predict molecular properties. Because of this, seven algorithms were picked for comparison. These were mean value, linear regression, linear ridged regression, tree regression, k-nearest neighbors, and support vector machines. The support vector machines method was then broken down into two methods, one using a gaussian kernel, and the other using a laplacian kernel. All of the algorithms used were \hl{(000000000) from scikit-learn [CITE]}. The mean value method is used as a baseline for all of the other methods with its interpretation being the error that would be gotten by just guessing.


\hl{????}
\subsection{Mean Value}
\subsection{Linear Regression}
\subsection{Linear Ridge Regression}
\subsection{Tree Regression}
\subsection{k-Nearest Neighbors}
\subsection{Support Vector Machines}
\subsubsection{Gaussian Kernel}
\subsubsection{Laplacian Kernel}
\hl{????}



 \begin{figure}[H]
   \begin{center}
     \includegraphics[width=5in]{figures/surf.png}
   \end{center}
   \caption{Example scan of test errors (eV) as a function of SVM parameters C and $\gamma$. The minimum located in the center corresponding to the ideal pair of parameter values to use for the machine learning model.}
   \label{fig:scan}
 \end{figure}

\section{Procedure}

Roughly 1100 benzobisazole structures were picked to to cover as many combinations of cores, sides, and options as possible. In terms of Aryl triplets, all of the structures comprised of at most 4 triplets per side. (This allowed for a constant vector size for the naive feature vector.) (Many of these structures were from prior studies of different features of benzobisazoles) Once they were built the structures were optimized using Density Functional Theory (DFT) with B3LYP/6-31g*. After the optimization, the molecules were then ran with Time Dependent Density Functional Theory (TDDFT) with B3LYP/6-31g*. Beyond the 1100 benzobisazole structures, about 1000 other structures were collected for comparison. The DFT calculations were done using the computing cluster Marcy from Bucknell and Gaussian 09. The machine learning algorithms used were the ones implemented in the \hl{Scikit-Learn Python Library}.

Once all the calculations were completed, the resulting log files were parsed for HOMO, LUMO, and Band Gap values. From there, the first three feature vectors were constructed (Naive, Decay, Decay with Distance Correction) and the resulting vectors were stored with the parsed log data.

Once all the data and feature vectors were collected, they were feed into the machine learning algorithms. For each feature vector, property, method set the methods relevant parameters were optimized (C, $\gamma$, max depth, $\alpha$, n neighbors). Then using these optimized parameters, the machine learning models were trained using k-folds for cross validation, specifically using 10 folds [CITE]. All of the results were then tabulated and compared based on their mean absolute errors (MAE) calculated as the distance the ML predicted value was from DFT calculated value. All of the machine learning calculations were done on a single consumer quad core computer.

\section{Results}

 \begin{figure}[H]
   \begin{center}
     \includegraphics[width=5.5in]{figures/allbars2.pdf}
   \end{center}
   \caption{Results. The far left column of each of the graphs can be seen as a baseline value for the set. So from there, any values that are lower than that value are better (with the optimal value being 0.0 eV). For the numerical values, refer to the Appendix.}
   \label{fig:results}
 \end{figure}

\section{Conclusions}

Overall, the support vector machines with Gaussian kernel had the best performance across all of the goal features and all of the feature vectors. This matches similarly to the results seen by \hl{.... [CITE]}; however, they found that the laplacian kernel performed the best. The suspected reason for the difference in the two results is thought to be because their method involved single atoms as features, whereas this method was dealing with entire functional groups. The smoothness of the gaussian distribution is probably a better estimator in that (especially with something like a phenyl ring). The optimized parameters for the method were found to be $C \approx 10$ and $\gamma \approx 0.05$.

For the feature vectors, the decay was the best performing feature vector with the parameters $A = 1, H = 1, p = 1$. \hl{(0.71)} As well as having a better overall performance, it is also much faster than the naive feature vector because it has fewer components. This increased speed could especially be seen when training the data.
With the addition of predicted HOMO/LUMO values for the decay feature vector, the performance for estimating the goal features increased by about 25\%.

Unexpectedly, the decay function with distance corrections did not perform any better than the regular decay feature vector. In theory, this should have worked better with heterogeneous side chains, but its use seemed to only increase the MAE. We think that the cause of this is due to the lack of sampling of largely heterogeneous side chains. So perhaps, if the same tests were done with only heterogeneous side chains adding the correction factor might decrease the total error.

There were a few anomalies in the results. The most noticeable is the long tail distribution of errors. As one might expect, structures where features overlap ($A\_TON\_4ed\_A$, 0.5 eV) there were higher MAE values. Unfortunately, a few molecules within the set had unusually high band gap errors (ex: $8A\_TON\_A\_A$, 1.1 eV) and we have no explanation for why that occurred.

To run the original DFT and TDDFT calculations for these molecules took about 35,0000 CPU hours. Calculating the same properties using these machine learning methods took about 4 seconds for each property for all 1100 structures. Because of the way that machine learning algorithms work, the majority of the time spent for each of these calculations is spent in the training phase; fortunately, this calculation is only required once so by . Once the models have been trained, these calculations are often reduced to just matrix multiplications leading to it taking just 0.1 seconds to calculate each property for all 1100 structures.


\section{Future Work}

For possible future work, the first thing to do would be to collect more data. For this study, the majority of the structures were monomers with 1-4 rings out from the core. This could be expanded to include larger polymers and monomers, especially with an emphasis on having more heterogeneous side chains. The functional groups could also be expanded to include things such as furan, pyrrole, or other conjugated systems.

None of the feature vectors take into account the Kuhn expression for polymer chains. Because many of these machine learning algorithms use linear models to fit, they do not accurately represent the nonlinear decay of influence seen in long polymers. This leads to high errors when $n>1$ or $m>1$. The problem is not as pronounced when using SVM with Gaussian kernels because of how they decay \hl{(?)}. \hl{Some preliminary work was done with predicting n=1,2,3,4 and then fitting the Kuhn expression for long polymers. This additional correction seems to be a good way to account for polymerization of benzobisazoles.}


% \begin{table}[H]
%   \centering
%   \caption{TON\_n20}
%   \begin{tabular}{lrrr}
%             Type      & HOMO (eV) & LUMO (eV) & Band Gap (eV) \\
%     \hline\hline
%             Predicted & -5.59 & -2.36 & 2.73 \\
%             Corrected & -5.99 & -3.36 & 2.13 \\
%             Actual    & -5.96 & -3.48 & 2.17 \\
%     \hline\hline
%   \end{tabular}
%   \label{tab:TONn20}
% \end{table}

% \begin{table}[H]
%   \centering
%   \caption{TON\_m20}
%   \begin{tabular}{lrrr}
%             Type      & HOMO (eV) & LUMO (eV) & Band Gap (eV) \\
%     \hline\hline
%             Predicted & -5.61 & -2.34 & 2.73 \\
%             Corrected & -5.84 & -2.20 & 3.10 \\
%             Actual    & -5.73 & -2.16 & 3.14 \\
%     \hline\hline
%   \end{tabular}
%   \label{tab:TONm20}
% \end{table}

Another possible expansion of work would be to test with more properties of the molecules such as total energy, dipole, and etc. \hl{preliminary results have shown that the naive feature vector works the best with predicting total energy (117 hartree MAE with svm, 2800 hartree MAE with mean) (about 2x as effective). This could be because the total energy is more dependent on the structure as a whole rather than the localization to the core.}

As an end goal, it would be nice to generalize these machine learning methods such that they could be used for any structure. \hl{While the} but perhaps there is a way to do it based off localized functional groups.

The methods presented here would be used as a first line filter to rule out large amounts molecular structures that do not meet the desired requirements such that more time could be allocated only on the structures that show the most promise instead of relying on intuition. As can be seen by these methods speeds and relative accuracy, these machine learning methods work really well for this task. In the same amount of time that one could calculate a property of a structure using DFT the same property can be predicted here for millions of structures. From those structures one could imagine taking the best 1\% of the structures and running progressively more accurate methods.

\hl{In addition to the machine learning methods used in this study, there was also an attempt to use Neural Networks for the predictions. Unfortunately, the results were not satisfactory relative to the amount of time required (0.20 eV MAE). Drastic changes to the network made minimal differences to the final error. This is attributed to inexperience with proper methodologies with Neural Networks.}
\hl{Also, the methods mentioned in }\cite{hansen_assessment_2013} \hl{were tested and the results were not as good as any of the other name based features (0.25-0.35 eV). In addition to the accuracy problems, it had severe performance problems because it relys on $O(n^2)$ elements in each feature vector where $n$ is the number of atoms in the structure. To save time, only structures with $<45$ atoms total were tested, leaving about 450 structures.}

\nocite{ruddigkeit_enumeration_2012}
\nocite{darley_beyond_2008}
\nocite{frisch_gaussian_2009}
\nocite{montavon_machine_2013}
\nocite{handley_dynamically_2009}
\nocite{hansen_assessment_2013}
\nocite{schutt_how_2013}
\nocite{reymond_exploring_2012}
\nocite{whitfield_computational_2013}
\nocite{hansen_assessment_2013-1}
\nocite{ra_dft_2008}
\nocite{martell_assessment_1997}
\nocite{montavon_learning_2012}
\nocite{domingos_few_2012}
\nocite{martin_benchmark_1997}
\nocite{handley_potential_2010}
\nocite{rupp_fast_2012}

\bibliography{paper}
\newpage
\appendix

\section{Results}

\begin{table}[H]
  \centering
  \caption{Band Gap Results}
  \begin{tabular}{llr}
            Feature Vector     & Method       & MAE (eV)                \\
    \hline\hline
    \multirow{7}{*}{Naive}     & Mean Value   & 0.5051 $\pm$ 0.044 \\
                               & Linear       & 0.2628 $\pm$ 0.019 \\
                               & Linear Ridge & 0.2571 $\pm$ 0.020 \\
                               & Tree         & 0.1429 $\pm$ 0.016 \\
                               & k-NN         & 0.2096 $\pm$ 0.018 \\
                               & SVM Gauss    & \textbf{0.1355 $\pm$ 0.011} \\
                               & SVM Laplace  & 0.1681 $\pm$ 0.013 \\
    \hline
    \multirow{7}{*}{Decay}     & Mean Value   & 0.5051 $\pm$ 0.044 \\
                               & Linear       & 0.2531 $\pm$ 0.022 \\
                               & Linear Ridge & 0.2506 $\pm$ 0.023 \\
                               & Tree         & 0.1318 $\pm$ 0.018 \\
                               & k-NN         & 0.2112 $\pm$ 0.017 \\
                               & SVM Gauss    & \textbf{0.1297 $\pm$ 0.011} \\
                               & SVM Laplace  & 0.1591 $\pm$ 0.011 \\
    \hline
    \multirow{7}{*}{Decay with Length Corrections} & Mean Value   & 0.5051 $\pm$ 0.044 \\
                               & Linear       & 0.2485 $\pm$ 0.020 \\
                               & Linear Ridge & 0.2466 $\pm$ 0.020 \\
                               & Tree         & 0.1492 $\pm$ 0.029 \\
                               & k-NN         & 0.2169 $\pm$ 0.013 \\
                               & SVM Gauss    & \textbf{0.1314 $\pm$ 0.011} \\
                               & SVM Laplace  & 0.1631 $\pm$ 0.011 \\
    \hline
    \multirow{7}{*}{Decay with Predictions} & Mean Value   & 0.5051 $\pm$ 0.044 \\
                               & Linear       & 0.1211 $\pm$ 0.011 \\
                               & Linear Ridge & 0.1174 $\pm$ 0.010 \\
                               & Tree         & 0.1369 $\pm$ 0.012 \\
                               & k-NN         & 0.1724 $\pm$ 0.015 \\
                               & SVM Gauss    & \textbf{0.1004 $\pm$ 0.010} \\
                               & SVM Laplace  & 0.1154 $\pm$ 0.009 \\
    \hline\hline
  \end{tabular}
  \label{tab:gapresults}
\end{table}




% dummy           0.5051634571 $\pm$ 0.0445987177
% linear          0.2628013865 $\pm$ 0.0191410131
% linear ridge    0.2571941447 $\pm$ 0.0201764172
% tree            0.1429020978 $\pm$ 0.0167680834
% neighbors       0.2096637025 $\pm$ 0.0181147991
% svm gauss       0.1355760073 $\pm$ 0.0119249721
% svm laplace     0.1681324515 $\pm$ 0.0130277607

% dummy           0.5051634571 $\pm$ 0.0445987177
% linear          0.2531408695 $\pm$ 0.0224904198
% linear ridge    0.2506620918 $\pm$ 0.0231378900
% tree            0.1318830228 $\pm$ 0.0188910719
% neighbors       0.2112675515 $\pm$ 0.0170114767
% svm gauss       0.1297476028 $\pm$ 0.0110853891
% svm laplace     0.1591951178 $\pm$ 0.0113296220

% dummy           0.5051634571 $\pm$ 0.0445987177
% linear          0.2485031607 $\pm$ 0.0200054080
% linear ridge    0.2466766131 $\pm$ 0.0209252773
% tree            0.1492425133 $\pm$ 0.0293509142
% neighbors       0.2169500774 $\pm$ 0.0136509896
% svm gauss       0.1314727141 $\pm$ 0.0116959757
% svm laplace     0.1631757401 $\pm$ 0.0117913187

% dummy           0.5051634571 $\pm$ 0.0445987177
% linear          0.1211359604 $\pm$ 0.0116286996
% linear ridge    0.1174973321 $\pm$ 0.0101995097
% tree            0.1369228897 $\pm$ 0.0123968003
% neighbors       0.1724752783 $\pm$ 0.0152423160
% svm gauss       0.1004123381 $\pm$ 0.0106025428
% svm laplace     0.1154254876 $\pm$ 0.0096740880


\begin{table}[H]
  \centering
  \caption{HOMO Results}
  \begin{tabular}{llr}
            Feature Vector     & Method       & MAE (eV)                \\
    \hline\hline
    \multirow{7}{*}{Naive} & Mean Value   & 0.4330 $\pm$ 0.038 \\
                               & Linear       & 0.2002 $\pm$ 0.024 \\
                               & Linear Ridge & 0.1949 $\pm$ 0.024 \\
                               & Tree         & 0.1546 $\pm$ 0.012 \\
                               & k-NN         & 0.2449 $\pm$ 0.025 \\
                               & SVM Gauss    & \textbf{0.1126 $\pm$ 0.014} \\
                               & SVM Laplace  & 0.1557 $\pm$ 0.016 \\
    \hline
    \multirow{7}{*}{Decay}     & Mean Value   & 0.4330 $\pm$ 0.038 \\
                               & Linear       & 0.1868 $\pm$ 0.025 \\
                               & Linear Ridge & 0.1860 $\pm$ 0.025 \\
                               & Tree         & 0.1455 $\pm$ 0.017 \\
                               & k-NN         & 0.2549 $\pm$ 0.026 \\
                               & SVM Gauss    & \textbf{0.1069 $\pm$ 0.010} \\
                               & SVM Laplace  & 0.1431 $\pm$ 0.014 \\
    \hline
    \multirow{7}{*}{Decay with Length Corrections} & Mean Value   & 0.4330 $\pm$ 0.038 \\
                               & Linear       & 0.1883 $\pm$ 0.024 \\
                               & Linear Ridge & 0.1875 $\pm$ 0.025 \\
                               & Tree         & 0.1453 $\pm$ 0.021 \\
                               & k-NN         & 0.2622 $\pm$ 0.028 \\
                               & SVM Gauss    & \textbf{0.1082 $\pm$ 0.012} \\
                               & SVM Laplace  & 0.1457 $\pm$ 0.017 \\
    \hline
    \multirow{7}{*}{Decay with Predictions} & Mean Value   & 0.4330 $\pm$ 0.038 \\
                               & Linear       & 0.0904 $\pm$ 0.007 \\
                               & Linear Ridge & 0.0897 $\pm$ 0.008 \\
                               & Tree         & 0.1439 $\pm$ 0.016 \\
                               & k-NN         & 0.1734 $\pm$ 0.020 \\
                               & SVM Gauss    & \textbf{0.0852 $\pm$ 0.007} \\
                               & SVM Laplace  & 0.1001 $\pm$ 0.008 \\
    \hline\hline
  \end{tabular}
  \label{tab:homoresults}
\end{table}


% dummy            0.4330672042 $\pm$ 0.0380947474
% linear           0.2002858544 $\pm$ 0.0244903400
% linear ridge     0.1949752271 $\pm$ 0.0248602704
% tree             0.1546552205 $\pm$ 0.0122826022
% neighbors        0.2449855565 $\pm$ 0.0258103540
% svm gauss        0.1126612407 $\pm$ 0.0141583310
% svm laplace      0.1557953600 $\pm$ 0.0169683266

% dummy            0.4330672042 $\pm$ 0.0380947474
% linear           0.1868573168 $\pm$ 0.0252679509
% linear ridge     0.1860609806 $\pm$ 0.0257993800
% tree             0.1455811816 $\pm$ 0.0175405978
% neighbors        0.2549135699 $\pm$ 0.0261735083
% svm gauss        0.1069055686 $\pm$ 0.0100414699
% svm laplace      0.1431763121 $\pm$ 0.0145504211

% dummy            0.4330672042 $\pm$ 0.0380947474
% linear           0.1883473946 $\pm$ 0.0248143868
% linear ridge     0.1875977704 $\pm$ 0.0251635444
% tree             0.1453857318 $\pm$ 0.0214142873
% neighbors        0.2622268718 $\pm$ 0.0287591181
% svm gauss        0.1082111822 $\pm$ 0.0124536000
% svm laplace      0.1457133829 $\pm$ 0.0170146564

% dummy            0.4330672042 $\pm$ 0.0380947474
% linear           0.0904848003 $\pm$ 0.0077179117
% linear ridge     0.0897196018 $\pm$ 0.0085006159
% tree             0.1439614181 $\pm$ 0.0167746654
% neighbors        0.1734307394 $\pm$ 0.0202218099
% svm gauss        0.0852044604 $\pm$ 0.0072159851
% svm laplace      0.1001281598 $\pm$ 0.0086522943

\begin{table}[H]
  \centering
  \caption{LUMO Results}
  \begin{tabular}{llr}
            Feature Vector     & Method       & MAE (eV)                \\
    \hline\hline
    \multirow{7}{*}{Naive}     & Mean Value   & 0.4475 $\pm$ 0.037 \\
                               & Linear       & 0.1900 $\pm$ 0.028 \\
                               & Linear Ridge & 0.1862 $\pm$ 0.025 \\
                               & Tree         & 0.1529 $\pm$ 0.007 \\
                               & k-NN         & 0.2351 $\pm$ 0.019 \\
                               & SVM Gauss    & \textbf{0.1055 $\pm$ 0.010} \\
                               & SVM Laplace  & 0.1454 $\pm$ 0.014 \\
    \hline
    \multirow{7}{*}{Decay}     & Mean Value   & 0.4475 $\pm$ 0.037 \\
                               & Linear       & 0.1816 $\pm$ 0.026 \\
                               & Linear Ridge & 0.1801 $\pm$ 0.026 \\
                               & Tree         & 0.1510 $\pm$ 0.024 \\
                               & k-NN         & 0.2323 $\pm$ 0.017 \\
                               & SVM Gauss    & \textbf{0.1022 $\pm$ 0.015} \\
                               & SVM Laplace  & 0.1317 $\pm$ 0.012 \\
    \hline
    \multirow{7}{*}{Decay with Length Corrections} & Mean Value   & 0.4475 $\pm$ 0.037 \\
                               & Linear       & 0.1847 $\pm$ 0.027 \\
                               & Linear Ridge & 0.1828 $\pm$ 0.027 \\
                               & Tree         & 0.1540 $\pm$ 0.023 \\
                               & k-NN         & 0.2348 $\pm$ 0.014 \\
                               & SVM Gauss    & \textbf{0.1029 $\pm$ 0.012} \\
                               & SVM Laplace  & 0.1331 $\pm$ 0.012 \\
    \hline
    \multirow{7}{*}{Decay with Predictions} & Mean Value   & 0.4475 $\pm$ 0.037 \\
                               & Linear       & 0.1233 $\pm$ 0.012 \\
                               & Linear Ridge & 0.0877 $\pm$ 0.011 \\
                               & Tree         & 0.0863 $\pm$ 0.010 \\
                               & k-NN         & 0.1699 $\pm$ 0.017 \\
                               & SVM Gauss    & \textbf{0.0792 $\pm$ 0.009} \\
                               & SVM Laplace  & 0.0922 $\pm$ 0.009 \\
    \hline\hline
  \end{tabular}
  \label{tab:lumoresults}
\end{table}


% dummy           0.4475990362 $\pm$ 0.0379190939
% linear          0.1900266591 $\pm$ 0.0285711677
% linear ridge    0.1862466873 $\pm$ 0.0252330179
% tree            0.1529660715 $\pm$ 0.0077728697
% neighbors       0.2351363937 $\pm$ 0.0193158199
% svm gauss       0.1055791858 $\pm$ 0.0104366108
% svm laplace     0.1454523271 $\pm$ 0.0145551515

% dummy           0.4475990362 $\pm$ 0.0379190939
% linear          0.1816689922 $\pm$ 0.0263603308
% linear ridge    0.1801892946 $\pm$ 0.0269365682
% tree            0.1510884685 $\pm$ 0.0242985650
% neighbors       0.2323162704 $\pm$ 0.0179171316
% svm gauss       0.1022628994 $\pm$ 0.0150820066
% svm laplace     0.1317870230 $\pm$ 0.0121778503

% dummy           0.4475990362 $\pm$ 0.0379190939
% linear          0.1847361405 $\pm$ 0.0279060679
% linear ridge    0.1828131087 $\pm$ 0.0270449435
% tree            0.1540660334 $\pm$ 0.0230038065
% neighbors       0.2348623132 $\pm$ 0.0142070847
% svm gauss       0.1029487972 $\pm$ 0.0123167244
% svm laplace     0.1331720123 $\pm$ 0.0123964303

% dummy           0.4475990362 $\pm$ 0.0379190939
% tree            0.1233738710 $\pm$ 0.0122788181
% linear          0.0877481316 $\pm$ 0.0110845291
% linear ridge    0.0863736416 $\pm$ 0.0108794424
% neighbors       0.1699337745 $\pm$ 0.0170310968
% svm gauss       0.0792332669 $\pm$ 0.0091141118
% svm laplace     0.0922304705 $\pm$ 0.0092304161


\end{document}
